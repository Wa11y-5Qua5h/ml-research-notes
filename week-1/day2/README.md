# Week 1 â€“ Day 2
## Linear Regression from Scratch

### What I implemented
- Linear regression using NumPy
- Manual gradient descent (no autograd)
- Plotted loss vs iterations and data with learned line

### Key observations
- Loss decreases rapidly at first, then plateaus
- Learning rate strongly affects convergence
- Model captures the linear trend but not noise

### What confused me
- Why the gradient has a factor of 2
- How NumPy applies operations without explicit loops
- Why loss never reaches zero

### Questions to revisit
- How does this extend to classification?
- What changes when there are multiple features?
- How would regularization affect the loss curve?
